Naïve Bayes
Naïve Bayes is a probabilistic machine learning algorithm based on the Bayes Theorem, used in a wide
variety of classification tasks. In this article, we will understand the Naïve Bayes algorithm and all
essential concepts so that there is no room for doubts in understanding.

Bayes Theorem
Bayes’ Theorem is a simple mathematical formula used for calculating conditional probabilities.

Conditional probability is a measure of the probability of an event occurring given that another event
has (by assumption, presumption, assertion, or evidence) occurred.

The formula is;
<<<<<FORMULA>>>>>

Which tells us: how often A happens given that B happens, written P(A|B) also called posterior
probability, When we know: how often B happens given that A happens, written P(B|A) and how likely A is
on its own, written P(A) and how likely B is on its own, written P(B).

Assumptions Made by Naïve Bayes 
The fundamental Naïve Bayes assumption is that each feature makes an:
*independent
*equal
contribution to the outcome.

Types of Naïve Bayes Classifiers
1. Multinomial Naïve Bayes Classifier
Feature vectors represent the frequencies with which certain events have been generated by a multinomial
distribution. This is the event model typically used for document classification.

2. Bernoulli Naïve Bayes Classifier
In the multivariate Bernoulli event model, features are independent booleans (binary variables)
describing inputs. Like the multinomial model, this model is popular for document classification tasks,
where binary term occurrence (i.e. a word occurs in a document or not) features are used rather than
term frequencies (i.e. frequency of a word in the document).

3. Gaussian Naïve Bayes Classifier: 
 
In Gaussian Naïve Bayes, continuous values associated with each feature are assumed to be distributed
according to a Gaussian distribution (Normal distribution). When plotted, it gives a bell-shaped curve
which is symmetric about the mean of the feature values as shown below:
<<<<<<DIAGRAM>>>>>

The likelihood of the features is assumed to be Gaussian, hence, conditional probability is given by:
<<<<<FORMULA>>>>>